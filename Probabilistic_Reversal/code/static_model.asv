function data = static_model(theta,start_trial_lapse,n_trials_lapse)
%% Simulates data for the probabilistic reversal environment.


%% Initialize
% Parse parameters
beta = 8; 
lapse = 0;
recover = 1;
alpha = theta(1); 
stick = theta(2);
epsilon = theta(3);

nA = 2;
noise = 0.8; % probability that correct choice is rewarded
num_episodes = 10;
num_trials = 50;

engaged = 1; % 1 = attentive state; 0 = attention lapse
Q = ones(1,nA) / nA; % randomly initialize Q
data = zeros(num_episodes*num_trials,6); 

k = 0; % counter for data matrix

%% Simulate
for ep = 1:num_episodes % episode
    cor = 1 + rem(ep,2); % rem(ep,2): remainder of ep after division by 2
    side = 0; % side of stickiness; 1 = stick to A1; -1 = stick to A2
    for t = 1:num_trials
        k = k + 1;
        data(k,1) = engaged;
        if nargin > 1 && (ep-1)*num_trials+t > start_trial_lapse && ...
                (ep-1)*num_trials+t < start_trial_lapse + n_trials_lapse
            engaged = 0;
        end
        if engaged == 1
            pr = 1 / (1 + exp(beta * (Q(1) - Q(2) + side * stick))); % pr = probability of choosing action A1
            if rand < lapse, engaged = 0; end % lapse with a probability
        else
            pr = .5;
            if rand < recover, engaged = 1; end % recoverurn to engaged state with a probability
        end

        choice = 1 + (rand < pr); % choose A1 with probability pr
        if rand < epsilon
            choice = randsample([1 2], 1);
        end
        correct = choice == cor; 
        if rand < noise % reward correct choice with probability == noise
            r = correct;
        else
            r = 1 - correct;
        end
        if choice == 1 % stick to the side of previous action
            side = 1;
        else
            side = -1;
        end
        
        Q(choice) = Q(choice) + alpha(r + 1) * (r - Q(choice));
        data(k,2:6) = [cor choice correct r Q(cor)];
    end
end
end